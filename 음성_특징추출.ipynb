{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyN/93ZstZhCivU9BOvBtxqL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsjj10002/FackVoiceClassfication/blob/main/%EC%9D%8C%EC%84%B1_%ED%8A%B9%EC%A7%95%EC%B6%94%EC%B6%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 가져오기\n",
        "\n",
        "[The Fake-or-Real (FoR) Dataset](https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset/data)\n",
        "\n",
        "The Fake-or-Real (FoR) dataset is a collection of more than 195,000 utterances from real humans and computer generated speech. The dataset can be used to train classifiers to detect synthetic speech.\n",
        "The dataset is published in four versions: for-original, for-norm, for-2sec and for-rerec.\n",
        "\n",
        "*The  first version, named for-original, contains the files as collected from the speech sources, without any modification (balanced version).\n",
        "\n",
        "The second version, called for-norm, contains the same files, but balanced in terms of gender and class and normalized in terms of sample rate, volume and number of channels.\n",
        "\n",
        "The third one, named for-2sec is based on the second one, but with the files truncated at 2 seconds.\n",
        "\n",
        "The last version, named for-rerec, is a rerecorded version of the for-2second dataset, to simulate a scenario where an attacker sends an utterance through a voice channel (i.e. a phone call or a voice message).*\n"
      ],
      "metadata": {
        "id": "cbAvPonJr32M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2p_9sMhBmCcK"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d mohammedabdeldayem/the-fake-or-real-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHRhKFJImxav",
        "outputId": "49abbacc-9bf5-4faa-f71e-316b2b4fec4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset\n",
            "License(s): GNU Lesser General Public License 3.0\n",
            "Downloading the-fake-or-real-dataset.zip to /content\n",
            "100% 16.0G/16.0G [02:13<00:00, 145MB/s]\n",
            "100% 16.0G/16.0G [02:13<00:00, 129MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!mkdir the-fake-or-real-dataset\n",
        "!unzip -qq \"/content/the-fake-or-real-dataset.zip\" -d the-fake-or-real-dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S15F2k6lpZvM",
        "outputId": "1dcd9f13-7821-4e24-80ce-43e18996dd7a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 라이브러리 호출"
      ],
      "metadata": {
        "id": "Es1C9QopsoH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "vA8FITOssnnh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 라벨링\n",
        "데이터를 훈련, 시험, 검증 셋으로 나누고, 가짜와 실제 목소리로 라벨링함."
      ],
      "metadata": {
        "id": "pHD-kNjBrk0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(root_path):\n",
        "    data = []\n",
        "\n",
        "    # 각 버전별로 사용할 하위 폴더 이름 설정\n",
        "    \"\"\"\n",
        "    versions = {\n",
        "        'for-original': 'for-original',\n",
        "        'for-norm': 'for-norm',\n",
        "        'for-2sec': 'for-2seconds',\n",
        "        'for-rerec': 'for-rerecorded'\n",
        "    }\n",
        "\n",
        "    # 각 버전별로 반복\n",
        "    for version_key, version_folder in versions.items():\n",
        "        # 각 버전의 하위 폴더로 경로 생성\n",
        "        version_path = os.path.join(root_path, version_key, version_folder)\n",
        "\"\"\"\n",
        "        # 데이터셋의 세 가지 범주: testing, training, validation\n",
        "    categories = ['testing', 'training', 'validation']\n",
        "\n",
        "    for category in categories:\n",
        "        category_path = os.path.join(root_path, category)\n",
        "        types = ['fake', 'real']\n",
        "\n",
        "            # fake, real 폴더 내의 파일 탐색\n",
        "        for type_ in types:\n",
        "            type_path = os.path.join(category_path, type_)\n",
        "                # 해당 경로가 존재하는지 확인\n",
        "            if not os.path.exists(type_path):\n",
        "                print(f\"경로를 찾을 수 없습니다: {type_path}\")\n",
        "                continue\n",
        "\n",
        "            for filename in os.listdir(type_path):\n",
        "                    # .wav 파일만 처리\n",
        "                if filename.endswith('.wav'):\n",
        "                    file_path = os.path.join(type_path, filename)\n",
        "                        # 데이터 리스트에 파일 경로와 라벨 추가\n",
        "                    data.append({'path': file_path, 'label': type_})\n",
        "\n",
        "    # 데이터 프레임 생성\n",
        "    df = pd.DataFrame(data)\n",
        "    return df"
      ],
      "metadata": {
        "id": "V5oQUtptrftx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋의 기본 경로\n",
        "dataset_path = \"/content/the-fake-or-real-dataset/for-original/for-original\"\n",
        "\n",
        "# 데이터 로드\n",
        "df = load_data(dataset_path)\n",
        "\n",
        "# 데이터 프레임의 상위 5개 행 출력\n",
        "print(df.head())\n",
        "\n",
        "# 데이터 프레임의 기본 정보 출력\n",
        "print(df.info())\n",
        "\n",
        "# 각 라벨의 개수 확인\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# 데이터셋 분할: 트레인, 테스트, 검증 세트\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42)\n",
        "\n",
        "# 결과 확인\n",
        "print(\"Training set size:\", len(train_df))\n",
        "print(\"Validation set size:\", len(val_df))\n",
        "print(\"Testing set size:\", len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y_ikVwwtDRB",
        "outputId": "61ac17e6-a462-4c4e-afaf-5dec16ab662f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                path label\n",
            "0  /content/the-fake-or-real-dataset/for-original...  fake\n",
            "1  /content/the-fake-or-real-dataset/for-original...  fake\n",
            "2  /content/the-fake-or-real-dataset/for-original...  fake\n",
            "3  /content/the-fake-or-real-dataset/for-original...  fake\n",
            "4  /content/the-fake-or-real-dataset/for-original...  fake\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40913 entries, 0 to 40912\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   path    40913 non-null  object\n",
            " 1   label   40913 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 639.4+ KB\n",
            "None\n",
            "label\n",
            "real    34605\n",
            "fake     6308\n",
            "Name: count, dtype: int64\n",
            "Training set size: 24547\n",
            "Validation set size: 8183\n",
            "Testing set size: 8183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 특징 추출(MFCC, 스펙트럼 대역, 제로 크로싱 레이트)"
      ],
      "metadata": {
        "id": "lcO-2CPbtWeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "from functools import partial\n",
        "def extract_features(file_path):\n",
        "    try:\n",
        "        # 파일에서 오디오 데이터 로드\n",
        "        audio, sample_rate = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # 오디오 데이터가 비어 있지 않은지 확인\n",
        "        if audio.size == 0:\n",
        "            print(f\"오디오 데이터가 비어 있습니다: {file_path}\")\n",
        "            return None\n",
        "\n",
        "        # MFCC 추출\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13), axis=1)\n",
        "\n",
        "        # 스펙트럼 대역 평균 추출\n",
        "        spectral_centroids = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sample_rate), axis=1)\n",
        "\n",
        "        # 제로 크로싱 레이트 평균 추출\n",
        "        zero_crossing_rates = np.mean(librosa.feature.zero_crossing_rate(audio), axis=1)\n",
        "\n",
        "        return mfccs, spectral_centroids, zero_crossing_rates\n",
        "    except Exception as e:\n",
        "        print(f\"파일 처리 중 에러가 발생했습니다: {file_path}, 에러: {e}\")\n",
        "        return None\n",
        "\n",
        "#병렬실행으로 실행 시간 단축\n",
        "def parallel_feature_extraction(file_paths):\n",
        "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
        "    features = pool.map(extract_features, file_paths)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return features"
      ],
      "metadata": {
        "id": "oyH6-zA_tLdR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 데이터셋에 대해 병렬 특징 추출 실행\n",
        "for dataset in [train_df, val_df, test_df]:\n",
        "    # 파일 경로 리스트에서 None 제외\n",
        "    file_paths = dataset['path'].dropna().tolist()\n",
        "    # 병렬 특징 추출\n",
        "    extracted_features = parallel_feature_extraction(file_paths)\n",
        "    # 결과 처리\n",
        "    features = pd.Series(extracted_features)\n",
        "    valid_features = features.dropna()\n",
        "    # 특징 데이터프레임 생성\n",
        "    feature_df = pd.DataFrame(valid_features.tolist(), columns=['mfcc', 'spectral_band', 'zero_crossing_rate'])\n",
        "    # 각 특징을 해당 데이터셋에 추가\n",
        "    dataset['mfcc'] = feature_df['mfcc']\n",
        "    dataset['spectral_band'] = feature_df['spectral_band']\n",
        "    dataset['zero_crossing_rate'] = feature_df['zero_crossing_rate']\n",
        "\n",
        "# 결과 확인\n",
        "print(train_df.head())\n",
        "\n",
        "# 데이터프레임을 CSV 파일로 저장\n",
        "train_df.to_csv('/content/TrainingDataset.csv', index=False)\n",
        "val_df.to_csv('/content/ValidationDataset.csv', index=False)\n",
        "test_df.to_csv('/content/TestingDataset.csv', index=False)\n",
        "\n",
        "# 저장된 파일 경로 출력\n",
        "print(\"파일이 저장되었습니다:\")\n",
        "print(\"/content/TrainingDataset.csv\")\n",
        "print(\"/content/ValidationDataset.csv\")\n",
        "print(\"/content/TestingDataset.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6FrwtVjtc_s",
        "outputId": "c1ee3dcc-2036-4a80-9f44-a51f3ccdf515"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    path label  \\\n",
            "26417  /content/the-fake-or-real-dataset/for-original...  real   \n",
            "31435  /content/the-fake-or-real-dataset/for-original...  real   \n",
            "24337  /content/the-fake-or-real-dataset/for-original...  real   \n",
            "2156   /content/the-fake-or-real-dataset/for-original...  fake   \n",
            "13290  /content/the-fake-or-real-dataset/for-original...  real   \n",
            "\n",
            "                                                    mfcc  \\\n",
            "26417                                                NaN   \n",
            "31435                                                NaN   \n",
            "24337  [-297.1028, 77.44087, 3.2898045, 8.396718, -20...   \n",
            "2156   [-309.6398, 91.91697, -7.0920734, 7.699547, -1...   \n",
            "13290  [-288.67917, 64.08434, -5.1230516, 11.13057, -...   \n",
            "\n",
            "              spectral_band     zero_crossing_rate  \n",
            "26417                   NaN                    NaN  \n",
            "31435                   NaN                    NaN  \n",
            "24337   [2657.135073871866]  [0.17655243514972144]  \n",
            "2156    [2276.301715645839]  [0.13309424975522194]  \n",
            "13290  [2792.4080753933486]  [0.17611533717105263]  \n",
            "파일이 저장되었습니다:\n",
            "/content/TrainingDataset.csv\n",
            "/content/ValidationDataset.csv\n",
            "/content/TestingDataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvCldTKcY9nn",
        "outputId": "c6a933cb-1966-4fb8-84ae-3f52abebe6f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    }
  ]
}